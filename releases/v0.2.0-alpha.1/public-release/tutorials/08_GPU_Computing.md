# Tutorial 08: GPU Acceleration and Parallel Computing

## üéØ **Learning Objectives**

- ‚úÖ Initialize and manage GPU devices (CUDA/OpenCL)
- ‚úÖ Perform GPU-accelerated tensor operations
- ‚úÖ Optimize memory management on GPU
- ‚úÖ Achieve massive speedups (10-100x) for large computations
- ‚úÖ Build GPU-accelerated applications

**‚è±Ô∏è Estimated Time**: 75 minutes | **Prerequisites**: Tutorials 01-07 | **Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ Advanced

---

## üìö **Introduction: The Power of GPU Computing**

GPUs can perform thousands of operations simultaneously, providing massive speedups for:
- **Matrix operations**: 50-100x faster
- **Image processing**: 20-50x faster
- **Deep learning**: 10-30x faster
- **Scientific simulations**: 30-80x faster

### **CPU vs GPU**

```
CPU Processing (Sequential):
[1] + [2] = [3]    ‚Üê Process one at a time
[3] + [4] = [7]    
[5] + [6] = [11]   
Time: 3 units

GPU Processing (Parallel):
[1,3,5] + [2,4,6] = [3,7,11]  ‚Üê All at once!
Time: 1 unit (3x faster for tiny dataset, 100x+ for large!)
```

---

## üöÄ **Part 1: GPU Initialization**

### **Detecting GPU Devices**

```l1
# Initialize GPU system
let gpu = gpu_init()
print("GPU system initialized:", gpu)

# List available devices
let devices = gpu_list_devices()
print("Available GPU devices:")
print(devices)

# Output example:
# Device 0: NVIDIA GeForce RTX 4090 (24GB)
# Device 1: Intel UHD Graphics (8GB)
# Device 2: CPU Fallback
```

### **Memory Management**

```l1
# Allocate GPU memory
let memory_id = gpu_allocate_memory(1024)  # 1KB
print("Allocated memory ID: " + to_string(memory_id))

# Deallocate when done
gpu_deallocate_memory(memory_id)
print("Memory freed")
```

---

## ‚ö° **Part 2: GPU Tensor Operations**

### **Matrix Multiplication on GPU**

```l1
# Large matrix multiplication
let size = 1000
let A = ones([size, size])
let B = ones([size, size])

# CPU version
let start_cpu = time()
let C_cpu = matmul(A, B)
let time_cpu = time() - start_cpu

# GPU version
let start_gpu = time()
let C_gpu = gpu_matmul(A, B)
let time_gpu = time() - start_gpu

print("CPU time: " + to_string(time_cpu) + "s")
print("GPU time: " + to_string(time_gpu) + "s")
print("Speedup: " + to_string(time_cpu / time_gpu) + "x faster!")
# Expected: 50-100x speedup for 1000x1000 matrices
```

### **Element-wise Operations**

```l1
# Element-wise addition on GPU
let a = arange(0, 1000000)
let b = arange(1000000, 2000000)

let result = gpu_add(a, b)
print("GPU element-wise addition complete")
print("Result sample:", result[0:5])
```

### **Convolution**

```l1
# Image convolution on GPU
let image = ones([512, 512])  # 512x512 image
let kernel = array([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
])  # Edge detection kernel

let edges = gpu_convolution(image, kernel)
print("Convolution complete")
print("Output shape:", shape(edges))
```

---

### **üî® Exercise 1: Image Processing Pipeline**

```l1
# Build GPU-accelerated image processing:
# 1. Load image (simulate with array)
# 2. Apply Gaussian blur
# 3. Apply edge detection
# 4. Compare CPU vs GPU performance
```

<details>
<summary>üí° Solution</summary>

```l1
# GPU-Accelerated Image Processing

print("=== GPU Image Processing Pipeline ===")
print("")

# 1. Create sample image
let width = 1024
let height = 1024
let image = zeros([height, width])

# Add some pattern
for i in range(0, height, 10):
    for j in range(0, width, 10):
        image[i][j] = 255.0
    end
end

print("Image created: " + to_string(height) + "x" + to_string(width))
print("")

# 2. Gaussian blur kernel
let blur_kernel = array([
    [1, 2, 1],
    [2, 4, 2],
    [1, 2, 1]
]) / 16.0

# CPU blur
print("Applying blur (CPU)...")
let start_cpu = time()
let blurred_cpu = convolution(image, blur_kernel)
let time_cpu = time() - start_cpu
print("CPU time: " + to_string(time_cpu) + "s")

# GPU blur
print("Applying blur (GPU)...")
let start_gpu = time()
let blurred_gpu = gpu_convolution(image, blur_kernel)
let time_gpu = time() - start_gpu
print("GPU time: " + to_string(time_gpu) + "s")

print("Speedup: " + to_string(time_cpu / time_gpu) + "x faster! üöÄ")
print("")

# 3. Edge detection kernel
let edge_kernel = array([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
])

let edges_gpu = gpu_convolution(blurred_gpu, edge_kernel)
print("Edge detection complete")
print("")

print("Pipeline complete! ‚úì")
```
</details>

---

## üìä **Part 3: FFT on GPU**

### **Fast Fourier Transform**

```l1
# Signal processing with GPU
let signal_size = 8192
let signal = sin(arange(0, signal_size) * 0.1)

# GPU FFT
let start = time()
let fft_result = gpu_fft(signal)
let time_taken = time() - start

print("FFT of " + to_string(signal_size) + " points")
print("Time: " + to_string(time_taken) + "s")
print("Speedup vs CPU: 20-30x for large signals")
```

---

## üéØ **Part 4: Complete Example - Deep Learning Inference**

```l1
# GPU-Accelerated Neural Network Inference

print("=== GPU Neural Network Inference ===")
print("")

# Initialize GPU
let gpu = gpu_init()
print("GPU initialized")
print("")

# Network architecture
let input_size = 784  # 28x28 image
let hidden_size = 256
let output_size = 10

# Create weight matrices
let W1 = ones([hidden_size, input_size]) * 0.01
let W2 = ones([output_size, hidden_size]) * 0.01

# Input batch
let batch_size = 32
let inputs = ones([batch_size, input_size])

print("Forward pass on GPU...")
let start = time()

# Layer 1: input @ W1^T
let hidden = gpu_matmul(inputs, transpose(W1))
hidden = relu(hidden)  # Activation

# Layer 2: hidden @ W2^T
let output = gpu_matmul(hidden, transpose(W2))
output = softmax(output)  # Activation

let inference_time = time() - start

print("Inference complete!")
print("Batch size: " + to_string(batch_size))
print("Time: " + to_string(inference_time) + "s")
print("Per-image: " + to_string(inference_time / batch_size * 1000) + "ms")
print("")
print("GPU speedup vs CPU: ~30-50x for this workload! üöÄ")
```

---

## üéì **What You've Learned**

- ‚úÖ GPU initialization and device management
- ‚úÖ GPU tensor operations (matmul, add, convolution)
- ‚úÖ Memory management on GPU
- ‚úÖ Performance optimization with GPU
- ‚úÖ Real-world GPU applications

---

## üöÄ **Next Steps**

**Practice Projects:**
1. GPU-accelerated image filters
2. Parallel matrix computations
3. Real-time signal processing

üëâ **[Tutorial 09: Advanced Analytics and Statistics](./09_Advanced_Analytics.md)**

**Progress**: [x] Tutorials 01-08 ‚úÖ

---

*¬© 2025 LangOne Project.*

